{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append('/home/i008/sentiment-analysis-of-tweets-using-emoticons')\n",
    "from src.data_preparation.read_data import read_csv\n",
    "from src.data_preprocessing.clean_tweet import remove_pattern_from_tweet,cleanString,preprocess_data\n",
    "from src.features.spacy_helpers import setup_spacy, add_spacy_features\n",
    "from src.features.prepare_target import add_polarity_scores,sum_polarity,prepare_target\n",
    "from src.models.model import random_search_best_estimator, final_model, save_model\n",
    "from src.features.train_test_preparation import average_tweet_vectorizer, y_X_preparation, train_test_preparation_for_model\n",
    "from src.data_preprocessing.clean_dataframe import remove_dup_tokens, unique_tokens, additional_processing,downsample_target\n",
    "from src.data_preparation.reference_emoji_list import prepare_reference_emoji_list\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression,Lasso\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kasia/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/home/kasia/sentiment-analysis-of-tweets-using-emoticons/src/features/spacy_helpers.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokens'] = collect_tweets\n",
      "/home/kasia/sentiment-analysis-of-tweets-using-emoticons/src/features/spacy_helpers.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['pos'] = collect_pos\n",
      "/home/kasia/sentiment-analysis-of-tweets-using-emoticons/src/features/spacy_helpers.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['adj'] = collect_pos_adj\n",
      "/home/kasia/sentiment-analysis-of-tweets-using-emoticons/src/features/spacy_helpers.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['emoji'] = collect_emoji\n",
      "/home/kasia/sentiment-analysis-of-tweets-using-emoticons/src/features/spacy_helpers.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['unique_emoji'] = unique_tokens(df.emoji)\n"
     ]
    }
   ],
   "source": [
    "nlp = setup_spacy()\n",
    "df = read_csv(\"data/raw/raw_tweets.csv\")\n",
    "df = preprocess_data(df)\n",
    "df = add_spacy_features(df=df, nlp=nlp)\n",
    "df = additional_processing(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tidy_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos</th>\n",
       "      <th>adj</th>\n",
       "      <th>emoji</th>\n",
       "      <th>unique_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @sphesihle_cm: 6LACK's catalogue is a perfe...</td>\n",
       "      <td>6LACK s catalogue is a perfect 10 🤞🏽</td>\n",
       "      <td>[6lack, s, catalogue, perfect, 10]</td>\n",
       "      <td>[SPACE, PROPN, PART, NOUN, AUX, DET, ADJ, NUM,...</td>\n",
       "      <td>[perfect]</td>\n",
       "      <td>[🤞🏽]</td>\n",
       "      <td>[🤞🏽]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RT @TallRico_: Niccas really do be having the ...</td>\n",
       "      <td>Niccas really do be having the fucking audaci...</td>\n",
       "      <td>[niccas, fucking, audacity]</td>\n",
       "      <td>[SPACE, PROPN, ADV, AUX, AUX, VERB, DET, ADJ, ...</td>\n",
       "      <td>[fucking]</td>\n",
       "      <td>[🤨]</td>\n",
       "      <td>[🤨]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RT @Nazaleen_: Don’t mind me, just adding to m...</td>\n",
       "      <td>Don’t mind me  just adding to my media 🌚</td>\n",
       "      <td>[mind, -pron-, add, -pron-, medium]</td>\n",
       "      <td>[SPACE, AUX, PART, VERB, PRON, SPACE, ADV, VER...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[🌚]</td>\n",
       "      <td>[🌚]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2 year old 😳😳 https://t.co/8QtoVjVl4e</td>\n",
       "      <td>2 year old 😳😳</td>\n",
       "      <td>[2, year, old]</td>\n",
       "      <td>[NUM, NOUN, PROPN, PROPN, PROPN]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[😳, 😳]</td>\n",
       "      <td>[😳]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RT @slitmyclitt: why is this my lactose intole...</td>\n",
       "      <td>why is this my lactose intolerant ass😩</td>\n",
       "      <td>[-pron-, lactose, intolerant, ass]</td>\n",
       "      <td>[SPACE, ADV, AUX, DET, DET, NOUN, NOUN, PROPN,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[😩]</td>\n",
       "      <td>[😩]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149972</th>\n",
       "      <td>RT @_ThatGuyFuller: He’s the coolest puppy 🥺 h...</td>\n",
       "      <td>He’s the coolest puppy 🥺</td>\n",
       "      <td>[-pron-, ’, cool, puppy]</td>\n",
       "      <td>[SPACE, PRON, VERB, DET, ADJ, NOUN, PUNCT]</td>\n",
       "      <td>[coolest]</td>\n",
       "      <td>[🥺]</td>\n",
       "      <td>[🥺]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149977</th>\n",
       "      <td>RT @joelean777: “kill em with success and they...</td>\n",
       "      <td>“kill em with success and they’ll never have ...</td>\n",
       "      <td>[kill, -pron-, success, -pron-, clue, -pron-, ...</td>\n",
       "      <td>[SPACE, PUNCT, VERB, PRON, ADP, NOUN, CCONJ, P...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[🔥]</td>\n",
       "      <td>[🔥]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149983</th>\n",
       "      <td>RT @JustSimon13: What's happening? ✌️😅 https:/...</td>\n",
       "      <td>What s happening  ✌️😅</td>\n",
       "      <td>[s, happen, ️]</td>\n",
       "      <td>[SPACE, DET, VERB, VERB, SPACE, NOUN, PROPN, P...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[✌, 😅]</td>\n",
       "      <td>[✌, 😅]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149986</th>\n",
       "      <td>@erigganewmoney I'm not a celebrity. Let's gro...</td>\n",
       "      <td>I m not a celebrity  Let s grow together  hit...</td>\n",
       "      <td>[-pron-, m, celebrity, let, s, grow, hit, foll...</td>\n",
       "      <td>[SPACE, PRON, VERB, PART, DET, NOUN, SPACE, VE...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[😘, 😘]</td>\n",
       "      <td>[😘]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149987</th>\n",
       "      <td>when he listens to your favorite song with you 🥰</td>\n",
       "      <td>when he listens to your favorite song with you 🥰</td>\n",
       "      <td>[-pron-, listen, -pron-, favorite, song, -pron-]</td>\n",
       "      <td>[ADV, PRON, VERB, ADP, DET, ADJ, NOUN, ADP, PR...</td>\n",
       "      <td>[favorite]</td>\n",
       "      <td>[🥰]</td>\n",
       "      <td>[🥰]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27567 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "5       RT @sphesihle_cm: 6LACK's catalogue is a perfe...   \n",
       "17      RT @TallRico_: Niccas really do be having the ...   \n",
       "18      RT @Nazaleen_: Don’t mind me, just adding to m...   \n",
       "32                  2 year old 😳😳 https://t.co/8QtoVjVl4e   \n",
       "37      RT @slitmyclitt: why is this my lactose intole...   \n",
       "...                                                   ...   \n",
       "149972  RT @_ThatGuyFuller: He’s the coolest puppy 🥺 h...   \n",
       "149977  RT @joelean777: “kill em with success and they...   \n",
       "149983  RT @JustSimon13: What's happening? ✌️😅 https:/...   \n",
       "149986  @erigganewmoney I'm not a celebrity. Let's gro...   \n",
       "149987   when he listens to your favorite song with you 🥰   \n",
       "\n",
       "                                                tidy_text  \\\n",
       "5                   6LACK s catalogue is a perfect 10 🤞🏽    \n",
       "17       Niccas really do be having the fucking audaci...   \n",
       "18              Don’t mind me  just adding to my media 🌚    \n",
       "32                                         2 year old 😳😳    \n",
       "37                why is this my lactose intolerant ass😩    \n",
       "...                                                   ...   \n",
       "149972                          He’s the coolest puppy 🥺    \n",
       "149977   “kill em with success and they’ll never have ...   \n",
       "149983                             What s happening  ✌️😅    \n",
       "149986   I m not a celebrity  Let s grow together  hit...   \n",
       "149987   when he listens to your favorite song with you 🥰   \n",
       "\n",
       "                                                   tokens  \\\n",
       "5                      [6lack, s, catalogue, perfect, 10]   \n",
       "17                            [niccas, fucking, audacity]   \n",
       "18                    [mind, -pron-, add, -pron-, medium]   \n",
       "32                                         [2, year, old]   \n",
       "37                     [-pron-, lactose, intolerant, ass]   \n",
       "...                                                   ...   \n",
       "149972                           [-pron-, ’, cool, puppy]   \n",
       "149977  [kill, -pron-, success, -pron-, clue, -pron-, ...   \n",
       "149983                                     [s, happen, ️]   \n",
       "149986  [-pron-, m, celebrity, let, s, grow, hit, foll...   \n",
       "149987   [-pron-, listen, -pron-, favorite, song, -pron-]   \n",
       "\n",
       "                                                      pos         adj   emoji  \\\n",
       "5       [SPACE, PROPN, PART, NOUN, AUX, DET, ADJ, NUM,...   [perfect]    [🤞🏽]   \n",
       "17      [SPACE, PROPN, ADV, AUX, AUX, VERB, DET, ADJ, ...   [fucking]     [🤨]   \n",
       "18      [SPACE, AUX, PART, VERB, PRON, SPACE, ADV, VER...          []     [🌚]   \n",
       "32                       [NUM, NOUN, PROPN, PROPN, PROPN]          []  [😳, 😳]   \n",
       "37      [SPACE, ADV, AUX, DET, DET, NOUN, NOUN, PROPN,...          []     [😩]   \n",
       "...                                                   ...         ...     ...   \n",
       "149972         [SPACE, PRON, VERB, DET, ADJ, NOUN, PUNCT]   [coolest]     [🥺]   \n",
       "149977  [SPACE, PUNCT, VERB, PRON, ADP, NOUN, CCONJ, P...          []     [🔥]   \n",
       "149983  [SPACE, DET, VERB, VERB, SPACE, NOUN, PROPN, P...          []  [✌, 😅]   \n",
       "149986  [SPACE, PRON, VERB, PART, DET, NOUN, SPACE, VE...          []  [😘, 😘]   \n",
       "149987  [ADV, PRON, VERB, ADP, DET, ADJ, NOUN, ADP, PR...  [favorite]     [🥰]   \n",
       "\n",
       "       unique_emoji  \n",
       "5              [🤞🏽]  \n",
       "17              [🤨]  \n",
       "18              [🌚]  \n",
       "32              [😳]  \n",
       "37              [😩]  \n",
       "...             ...  \n",
       "149972          [🥺]  \n",
       "149977          [🔥]  \n",
       "149983       [✌, 😅]  \n",
       "149986          [😘]  \n",
       "149987          [🥰]  \n",
       "\n",
       "[27567 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kasia/sentiment-analysis-of-tweets-using-emoticons/src/features/prepare_target.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sum_polarity_unique_emoji'] = df['polarity_for_unique_emoji'].map(sum)\n",
      "/home/kasia/sentiment-analysis-of-tweets-using-emoticons/src/features/prepare_target.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment_target'] = sentiment_target\n"
     ]
    }
   ],
   "source": [
    "emoji_dict = prepare_reference_emoji_list(nlp=nlp)\n",
    "df = add_polarity_scores(df, emoji_dict, emoji_column='unique_emoji')\n",
    "df = df[df.polarity_for_unique_emoji.map(bool)]\n",
    "df = sum_polarity(df)\n",
    "df = prepare_target(df)\n",
    "df = downsample_target(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tidy_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos</th>\n",
       "      <th>adj</th>\n",
       "      <th>emoji</th>\n",
       "      <th>unique_emoji</th>\n",
       "      <th>polarity_for_unique_emoji</th>\n",
       "      <th>sum_polarity_unique_emoji</th>\n",
       "      <th>sentiment_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @5nharris: I hate when big girls do skinny ...</td>\n",
       "      <td>I hate when big girls do skinny girl s    😡😡 ...</td>\n",
       "      <td>[-pron-, hate, big, girl, skinny, girl, s, lik...</td>\n",
       "      <td>[SPACE, PRON, VERB, ADV, ADJ, NOUN, AUX, ADJ, ...</td>\n",
       "      <td>[big, skinny, 😡]</td>\n",
       "      <td>[😡, 😡]</td>\n",
       "      <td>[😡]</td>\n",
       "      <td>[-4]</td>\n",
       "      <td>-4</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @wavyzaybo: So 42 dugg ex got on live and l...</td>\n",
       "      <td>So 42 dugg ex got on live and look what she s...</td>\n",
       "      <td>[42, dugg, ex, live, look, -pron-, dawg, lmaoo]</td>\n",
       "      <td>[SPACE, ADV, NUM, VERB, PROPN, VERB, ADP, ADV,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[😭, 😭]</td>\n",
       "      <td>[😭]</td>\n",
       "      <td>[-3]</td>\n",
       "      <td>-3</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @FuckYugi: Still cant’ believe this is the ...</td>\n",
       "      <td>Still cant’ believe this is the dub sometimes 😭</td>\n",
       "      <td>[believe, dub]</td>\n",
       "      <td>[SPACE, ADV, VERB, PART, PUNCT, VERB, DET, AUX...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[😭]</td>\n",
       "      <td>[😭]</td>\n",
       "      <td>[-3]</td>\n",
       "      <td>-3</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YOU FCKERS THATS 2018😭😭😭😭😭</td>\n",
       "      <td>YOU FCKERS THATS 2018😭😭😭😭😭</td>\n",
       "      <td>[fcker, thats, 2018]</td>\n",
       "      <td>[PRON, NOUN, PROPN, NUM, PROPN, PROPN, PROPN, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[😭, 😭, 😭, 😭, 😭]</td>\n",
       "      <td>[😭]</td>\n",
       "      <td>[-3]</td>\n",
       "      <td>-3</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Back to work tomorrow 😩</td>\n",
       "      <td>Back to work tomorrow 😩</td>\n",
       "      <td>[work, tomorrow]</td>\n",
       "      <td>[ADV, ADP, NOUN, NOUN, PROPN]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[😩]</td>\n",
       "      <td>[😩]</td>\n",
       "      <td>[-2]</td>\n",
       "      <td>-2</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7797</th>\n",
       "      <td>https://t.co/5aXzkSWeLX click on link to watch...</td>\n",
       "      <td>click on link to watch video about what this ...</td>\n",
       "      <td>[click, link, watch, video, photo, fun, night,...</td>\n",
       "      <td>[SPACE, PROPN, ADP, NOUN, PART, VERB, NOUN, AD...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[❤]</td>\n",
       "      <td>[❤]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7798</th>\n",
       "      <td>@MatthewEspinosa Happpppy Birthday 💓🦖 when we ...</td>\n",
       "      <td>Happpppy Birthday 💓🦖 when we will see each ot...</td>\n",
       "      <td>[happpppy, birthday, -pron-, boi]</td>\n",
       "      <td>[SPACE, PROPN, PROPN, NOUN, NOUN, ADV, PRON, V...</td>\n",
       "      <td>[other]</td>\n",
       "      <td>[💓, 🦖]</td>\n",
       "      <td>[💓, 🦖]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7799</th>\n",
       "      <td>RT @TeamUpRGC: @Jaytea2015 &amp;amp; The Majority ...</td>\n",
       "      <td>amp  The Majority Seems To Love It 👑Fam 💯💯💯...</td>\n",
       "      <td>[amp, majority, love, -pron-, fam, majority, -...</td>\n",
       "      <td>[SPACE, VERB, SPACE, DET, PROPN, VERB, PART, V...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[👑, 💯, 💯, 💯, 💯, 👑, 👍, 👍]</td>\n",
       "      <td>[👑, 💯, 👍]</td>\n",
       "      <td>[3, 2]</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7800</th>\n",
       "      <td>All Hail To 60Min\\n\\n62'\\n66'\\n67'\\n\\n🤣🤣🤣🤣🤣🤣🤣🤣...</td>\n",
       "      <td>All Hail To 60Min\\n\\n62 \\n66 \\n67 \\n\\n🤣🤣🤣🤣🤣🤣🤣🤣...</td>\n",
       "      <td>[hail, 60min, 62, 66, 67]</td>\n",
       "      <td>[DET, PROPN, ADP, NUM, SPACE, NUM, SPACE, NUM,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[🤣, 🤣, 🤣, 🤣, 🤣, 🤣, 🤣, 🤣, 🤣, 🤣, 🤣]</td>\n",
       "      <td>[🤣]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7801</th>\n",
       "      <td>RT @cctv_idiots: Poser 😎😂😂  https://t.co/CFPYg...</td>\n",
       "      <td>Poser 😎😂😂</td>\n",
       "      <td>[poser]</td>\n",
       "      <td>[SPACE, PROPN, NUM, X, X, SPACE]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[😎, 😂, 😂]</td>\n",
       "      <td>[😎, 😂]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7802 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     RT @5nharris: I hate when big girls do skinny ...   \n",
       "1     RT @wavyzaybo: So 42 dugg ex got on live and l...   \n",
       "2     RT @FuckYugi: Still cant’ believe this is the ...   \n",
       "3                            YOU FCKERS THATS 2018😭😭😭😭😭   \n",
       "4                               Back to work tomorrow 😩   \n",
       "...                                                 ...   \n",
       "7797  https://t.co/5aXzkSWeLX click on link to watch...   \n",
       "7798  @MatthewEspinosa Happpppy Birthday 💓🦖 when we ...   \n",
       "7799  RT @TeamUpRGC: @Jaytea2015 &amp; The Majority ...   \n",
       "7800  All Hail To 60Min\\n\\n62'\\n66'\\n67'\\n\\n🤣🤣🤣🤣🤣🤣🤣🤣...   \n",
       "7801  RT @cctv_idiots: Poser 😎😂😂  https://t.co/CFPYg...   \n",
       "\n",
       "                                              tidy_text  \\\n",
       "0      I hate when big girls do skinny girl s    😡😡 ...   \n",
       "1      So 42 dugg ex got on live and look what she s...   \n",
       "2      Still cant’ believe this is the dub sometimes 😭    \n",
       "3                            YOU FCKERS THATS 2018😭😭😭😭😭   \n",
       "4                               Back to work tomorrow 😩   \n",
       "...                                                 ...   \n",
       "7797   click on link to watch video about what this ...   \n",
       "7798   Happpppy Birthday 💓🦖 when we will see each ot...   \n",
       "7799     amp  The Majority Seems To Love It 👑Fam 💯💯💯...   \n",
       "7800  All Hail To 60Min\\n\\n62 \\n66 \\n67 \\n\\n🤣🤣🤣🤣🤣🤣🤣🤣...   \n",
       "7801                                        Poser 😎😂😂     \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [-pron-, hate, big, girl, skinny, girl, s, lik...   \n",
       "1       [42, dugg, ex, live, look, -pron-, dawg, lmaoo]   \n",
       "2                                        [believe, dub]   \n",
       "3                                  [fcker, thats, 2018]   \n",
       "4                                      [work, tomorrow]   \n",
       "...                                                 ...   \n",
       "7797  [click, link, watch, video, photo, fun, night,...   \n",
       "7798                  [happpppy, birthday, -pron-, boi]   \n",
       "7799  [amp, majority, love, -pron-, fam, majority, -...   \n",
       "7800                          [hail, 60min, 62, 66, 67]   \n",
       "7801                                            [poser]   \n",
       "\n",
       "                                                    pos               adj  \\\n",
       "0     [SPACE, PRON, VERB, ADV, ADJ, NOUN, AUX, ADJ, ...  [big, skinny, 😡]   \n",
       "1     [SPACE, ADV, NUM, VERB, PROPN, VERB, ADP, ADV,...                []   \n",
       "2     [SPACE, ADV, VERB, PART, PUNCT, VERB, DET, AUX...                []   \n",
       "3     [PRON, NOUN, PROPN, NUM, PROPN, PROPN, PROPN, ...                []   \n",
       "4                         [ADV, ADP, NOUN, NOUN, PROPN]                []   \n",
       "...                                                 ...               ...   \n",
       "7797  [SPACE, PROPN, ADP, NOUN, PART, VERB, NOUN, AD...                []   \n",
       "7798  [SPACE, PROPN, PROPN, NOUN, NOUN, ADV, PRON, V...           [other]   \n",
       "7799  [SPACE, VERB, SPACE, DET, PROPN, VERB, PART, V...                []   \n",
       "7800  [DET, PROPN, ADP, NUM, SPACE, NUM, SPACE, NUM,...                []   \n",
       "7801                   [SPACE, PROPN, NUM, X, X, SPACE]                []   \n",
       "\n",
       "                                  emoji unique_emoji  \\\n",
       "0                                [😡, 😡]          [😡]   \n",
       "1                                [😭, 😭]          [😭]   \n",
       "2                                   [😭]          [😭]   \n",
       "3                       [😭, 😭, 😭, 😭, 😭]          [😭]   \n",
       "4                                   [😩]          [😩]   \n",
       "...                                 ...          ...   \n",
       "7797                                [❤]          [❤]   \n",
       "7798                             [💓, 🦖]       [💓, 🦖]   \n",
       "7799           [👑, 💯, 💯, 💯, 💯, 👑, 👍, 👍]    [👑, 💯, 👍]   \n",
       "7800  [🤣, 🤣, 🤣, 🤣, 🤣, 🤣, 🤣, 🤣, 🤣, 🤣, 🤣]          [🤣]   \n",
       "7801                          [😎, 😂, 😂]       [😎, 😂]   \n",
       "\n",
       "     polarity_for_unique_emoji  sum_polarity_unique_emoji sentiment_target  \n",
       "0                         [-4]                         -4         negative  \n",
       "1                         [-3]                         -3         negative  \n",
       "2                         [-3]                         -3         negative  \n",
       "3                         [-3]                         -3         negative  \n",
       "4                         [-2]                         -2         negative  \n",
       "...                        ...                        ...              ...  \n",
       "7797                       [3]                          3         positive  \n",
       "7798                       [3]                          3         positive  \n",
       "7799                    [3, 2]                          5         positive  \n",
       "7800                       [4]                          4         positive  \n",
       "7801                    [1, 3]                          4         positive  \n",
       "\n",
       "[7802 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kasia/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 16 is smaller than n_iter=40. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=40. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "X,y = y_X_preparation(df)\n",
    "X_train, X_test, y_train, y_test = train_test_preparation_for_model(X,y)\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "knn = KNeighborsClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "sgd = SGDClassifier()\n",
    "\n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "param_grid =[#{'alpha': np.linspace(0.00001, 1, 40)},\n",
    "             {'penalty' : ['l1', 'l2'],'C' : [0,1,5,10],'solver' : ['lbfgs','liblinear']},\n",
    "             {'n_neighbors':[1,3,5,10]},\n",
    "             {'n_estimators':list(range(10,101,10)),'max_features':list(range(6,32,5))},\n",
    "             {'average': [True, False],'alpha': np.linspace(0.00001, 1, 40)}]\n",
    "model_list = [logistic, knn, rfc,sgd]\n",
    "\n",
    "grid_search, grid_results,results  = random_search_best_estimator(scorer,param_grid,model_list,X_train, X_test, y_train, y_test)   \n",
    "results = pd.DataFrame(results) \n",
    "results\n",
    "best_estimator = results['best_estimator'][results['best_score']==results['best_score'].max()].iloc[0]\n",
    "final_accuracy_test,final_accuracy_train,pred_test,pred_train = final_model(X_train, X_test, y_train, y_test,best_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp=setup_spacy() dodane w class src.features.train_test_preparation\n",
    "jak inaczej to rozwiazac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_accuracy_test,final_accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model = save_model('models','20200726_best_estimator.sav',best_estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
