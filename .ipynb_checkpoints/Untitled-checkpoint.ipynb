{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/i008/sentiment-analysis-of-tweets-using-emoticons')\n",
    "\n",
    "\n",
    "from spacymoji import Emoji\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression,Lasso\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stop_words\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import en_core_web_sm\n",
    "import string \n",
    "import itertools\n",
    "## for language detection\n",
    "import nltk\n",
    "import ast\n",
    "import re\n",
    "import csv\n",
    "import typing\n",
    "import pathlib\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i don't use this function\n",
    "def get_raw_path_file() -> typing.Union[str, pathlib.Path]:\n",
    "    \"\"\"Return the raw file path.\"\"\"\n",
    "    base_path = Path(__file__).parent\n",
    "    raw_tweets_csv_path = (base_path / \"../../../data/raw/raw_tweets.csv\").resolve()\n",
    "    return raw_tweets_csv_path\n",
    "\n",
    "def read_data(path):\n",
    "    #load data from csv to df\n",
    "    df = pd.read_csv(path)[:4000]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-cd9c4315eeb0>, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-cd9c4315eeb0>\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    def unique_tokens(emojis_column)\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# tweet peproecessing\n",
    "\n",
    "def remove_pattern_from_tweet(input_txt, pattern):\n",
    "    \"\"\"remove pattern from text like retweets, hashtags etc.\"\"\"\n",
    "    try:\n",
    "        r = re.findall(pattern, input_txt)\n",
    "        for i in r:\n",
    "            input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "        return input_txt  \n",
    "    except:\n",
    "        return None\n",
    "        \n",
    "\n",
    "def cleanString(strval):\n",
    "    \"\"\"remove punctation\"\"\"\n",
    "    return \"\".join(\" \" if i in string.punctuation else i for i in strval.strip(string.punctuation))\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"add column with preprocessed text without pattern\"\"\"\n",
    "    df['tidy_text'] = np.vectorize(remove_pattern_from_tweet)(df['text'], \"RT\\s.\\w+:\") #remove retweets\n",
    "    df['tidy_text'] = np.vectorize(remove_pattern_from_tweet)(df['tidy_text'], \"(@[A-Za-z0-9_]+)\") #remove twitter user name\n",
    "    df['tidy_text'] = np.vectorize(remove_pattern_from_tweet)(df['tidy_text'], \"(#\\w+)\")#remove hashtags\n",
    "    df['tidy_text'] = np.vectorize(remove_pattern_from_tweet)(df['tidy_text'], \"(?P<url>https?://[^\\s]+)\")\n",
    "    df['tidy_text'] = np.vectorize(cleanString)(df['tidy_text'])#remove punctation   \n",
    "    return df\n",
    "\n",
    "def remove_dup_tokens(tokens):\n",
    "    \"\"\"create two list first with all token and second only with unique token\"\"\"\n",
    "    collect = []\n",
    "    seen = []\n",
    "    for token in tokens:\n",
    "        if str(token) not in seen:\n",
    "            collect.append(token)\n",
    "        seen.append(str(token))\n",
    "    return collect\n",
    "\n",
    "\n",
    "def unique_tokens(emojis_column)    \n",
    "    \"\"\"remove duplication from columns with emoji\"\"\"\n",
    "    unique_emoji = []\n",
    "    for emoji in emojis_column:\n",
    "        collect = remove_dup_tokens(emoji)\n",
    "        unique_emoji.append(collect)\n",
    "    return unique_emoji  \n",
    "    \n",
    "\n",
    "def setup_spacy():\n",
    "    nlp = en_core_web_sm.load()\n",
    "    emoji = Emoji(nlp)\n",
    "    nlp.add_pipe(emoji, first=True)\n",
    "    return nlp\n",
    "\n",
    "def add_spacy_features(df=None, column='tidy_text', nlp=None):\n",
    "    \"\"\"add columns with features with spacy\"\"\"\n",
    "    \n",
    "    collect_tweets = []\n",
    "    collect_pos = []\n",
    "    collect_pos_adj = []\n",
    "    collect_emoji = []\n",
    "\n",
    "    is_corrupt = []\n",
    "    #prepare tweet\n",
    "    for tweet in df[column].values:\n",
    "        try:\n",
    "            tweet = nlp(tweet)\n",
    "            is_corrupt.append(False)\n",
    "        except:\n",
    "            is_corrupt.append(True)\n",
    "            continue\n",
    "\n",
    "        my_emoji = [word for word in tweet if word._.is_emoji]\n",
    "        my_tokens = [word for word in tweet if not word._.is_emoji]\n",
    "        my_tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON_\" \n",
    "                    else word.lower_ for word in my_tokens]\n",
    "        my_tokens = [word for word in my_tokens if\n",
    "                    word not in stop_words and word not in string.punctuation]\n",
    "\n",
    "        my_pos = [word.pos_ for word in tweet]\n",
    "        my_pos_adj = [word for word in tweet if word.pos_ == 'ADJ']\n",
    "        collect_tweets.append(my_tokens)\n",
    "        collect_pos.append(my_pos)\n",
    "        collect_pos_adj.append(my_pos_adj)\n",
    "        collect_emoji.append(my_emoji)\n",
    "\n",
    "    df = df[~np.array(is_corrupt)]\n",
    "    df['tokens'] = collect_tweets\n",
    "    df['pos'] = collect_pos\n",
    "    df['adj'] = collect_pos_adj\n",
    "    df['emoji'] = collect_emoji\n",
    "    df['unique_emoji'] = unique_tokens(df.emoji) \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def prepare_reference_emoji_list(fp='data/raw/emoji_list.csv', nlp=None):\n",
    "    \"\"\"preapre df with emoji in token form\"\"\"\n",
    "    emoji_list = pd.read_csv(fp, index_col=0)\n",
    "    emoji_token = [] \n",
    "    for x in emoji_list['symbol']:\n",
    "        sym = nlp(x)\n",
    "        emoji_token.append([word for word in sym if word._.is_emoji][0])\n",
    "    emoji_list['symbol'] = emoji_token\n",
    "\n",
    "    emoji_sym = list(emoji_list['symbol'])\n",
    "    emoji_sym = [str(e) for e in emoji_sym]\n",
    "    emoji_polarity= list(emoji_list['polarity'])\n",
    "    emoji_dict = dict(zip(emoji_sym,emoji_polarity))\n",
    "    \n",
    "    return emoji_dict\n",
    "\n",
    "def additional_processing(df):\n",
    "    \"\"\"remove rows without emoji\"\"\"\n",
    "    df = df[df['emoji'].map(len)!=0]\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_polarity_scores(df,emoji_dict, emoji_column='unique_emoji'):\n",
    "    \"\"\"prepare list with polarity of emoji from 'unique_emoji' column which are in emoji_dict\"\"\"\n",
    "    all_emoji_scores = []\n",
    "    for emoji_group in  df[emoji_column] :\n",
    "        tweet_emoji_score_list = []\n",
    "        for emoji in emoji_group:\n",
    "            if str(emoji) in emoji_dict:\n",
    "                tweet_emoji_score_list.append(emoji_dict.get(str(emoji)))\n",
    "            else:\n",
    "                pass\n",
    "        all_emoji_scores.append(tweet_emoji_score_list)\n",
    "    df['polarity_for_unique_emoji'] = all_emoji_scores    \n",
    "    return df\n",
    "\n",
    "def sum_polarity(df):\n",
    "    \"\"\"return sum of polarity for unique emoji in every row\"\"\"\n",
    "    df['sum_polarity_unique_emoji'] = df['polarity_for_unique_emoji'].map(sum)\n",
    "    fig = plt.figure()\n",
    "    plt.hist(df['sum_polarity_unique_emoji'].sort_values(),bins=20)\n",
    "    fig.savefig('polarity_hist.png')\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_target(df):\n",
    "    \"\"\"take sum of polarity and use conditions to prepare the target\"\"\"\n",
    "    sentiment_target = []\n",
    "    for polarity in df['sum_polarity_unique_emoji']:\n",
    "        if polarity <= -2:\n",
    "            sentiment_target.append('negative')\n",
    "        elif polarity >2:\n",
    "            sentiment_target.append('positive')\n",
    "        else:\n",
    "            sentiment_target.append(0)\n",
    "    df['sentiment_target'] = sentiment_target\n",
    "    df = df[df['sentiment_target']!=0]\n",
    "    return df\n",
    "\n",
    "def downsample_target(df):\n",
    "    \"\"\"return balanced dataframe\"\"\"\n",
    "    min_target_number = df['sentiment_target'].value_counts().min()\n",
    "    df = df.groupby('sentiment_target').apply(lambda x: x.sample(min_target_number, replace=False))\n",
    "    df = df.reset_index(drop=True)#.drop(['level_0','index'],axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# preparation for model\n",
    "\n",
    "def y_X_preparation(df):\n",
    "    \"return numeric y,X\"\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df['sentiment_target'])\n",
    "    X = df['tokens']\n",
    "    return X,y\n",
    "\n",
    "# Average Tweet Vectorizer\n",
    "\n",
    "#https://www.kaggle.com/zackakil/nlp-using-word-vectors-with-spacy-cldspn\n",
    "\n",
    "class average_tweet_vectorizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X):\n",
    "        word_vectorize = []\n",
    "        for i,token in enumerate(X):\n",
    "            word_vector_list = [nlp(word).vector for word in token]\n",
    "            average_word_vector = np.mean(word_vector_list, axis=0)\n",
    "            word_vectorize.append(average_word_vector.sum())\n",
    "        return np.nan_to_num(np.array(word_vectorize).reshape(-1,1))\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "def train_test_preparation_for_model(X,y):\n",
    "    \"\"\"prepare numeric features using Features Union and two classes\"\"\"\n",
    "    X = X.apply(lambda x: ' '.join(x))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "       ('average_tweet_vectorizer', average_tweet_vectorizer()),\n",
    "      ('tfidf',TfidfVectorizer())  \n",
    "    ]))])\n",
    "    X_train = pipeline.fit_transform(X_train)\n",
    "    X_test = pipeline.transform(X_test)\n",
    "    return X_train, X_test, y_train, y_test \n",
    "\n",
    "def random_search_best_estimator(scorer,param_grid,model_list,X_train, X_test, y_train, y_test):\n",
    "    \"\"\"return best estimator\"\"\"\n",
    "    results = []\n",
    "    for c,model in enumerate(model_list):\n",
    "        grid_search = RandomizedSearchCV(model,param_grid[c],cv=5,n_iter=40,scoring=scorer)\n",
    "        grid_search.fit(X_train,y_train)\n",
    "        grid_results = grid_search.cv_results_\n",
    "        results.append({'model_name':str(model_list[c]).split('X_train(')[0],'best_param':grid_search.best_params_,'best_score':grid_search.best_score_,'best_estimator':grid_search.best_estimator_})\n",
    "    return grid_search, grid_results,results\n",
    "\n",
    "def final_model(X_train, X_test, y_train, y_test,best_estimator):\n",
    "    \"return model for best estimator\"\n",
    "\n",
    "    final_model = Pipeline([\n",
    "    ('best_estimator', best_estimator,)\n",
    "    ])\n",
    "\n",
    "    # Fit on train\n",
    "    final_model.fit(X_train,y_train)\n",
    "    # Predict on test\n",
    "    pred_test = final_model.predict(X_test)\n",
    "    pred_train = final_model.predict(X_train)\n",
    "    final_accuracy_train = accuracy_score(y_train,pred_train)\n",
    "    final_accuracy_test = accuracy_score(y_test,pred_test)\n",
    "    # Return\n",
    "    return final_accuracy_test,final_accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kasia/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD4CAYAAAANbUbJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAARDklEQVR4nO3df4xlZX3H8fenoDb+BNxBt4BdMEiiTbuYCbW1GlqsAhrRRi2bRqnarqSSaGwTURM1GhP8gaamLWSpG7FRREUqUaxsqZU0KeqAy7oICEtXXdnujmLQRmO7+O0f94xeljs7lzn3zszyvF/JzT33Oc+55zvn3vuZM88590yqCknSw9uvrXYBkqTpM+wlqQGGvSQ1wLCXpAYY9pLUgCNXuwCAdevW1YYNG1a7DEk6rNx0000/qKqZcfquibDfsGEDc3Nzq12GJB1Wknxn3L4O40hSAwx7SWqAYS9JDVgy7JNsTbI/yc6htiuTbO9uu5Ns79o3JPnZ0LxLp1m8JGk84xyg/Sjwd8DHFhqq6k8XppNcDNw31H9XVW2cVIGSpP6WDPuquiHJhlHzkgR4BfBHky1LkjRJfcfsnwPsq6o7h9pOTPKNJF9J8pzFFkyyOclckrn5+fmeZUiSDqVv2G8Crhh6vBd4SlWdCrwJ+ESSx49asKq2VNVsVc3OzIz1nQBJ0jItO+yTHAn8CXDlQltV/byqfthN3wTsAp7Wt0hJUj99vkH7POD2qtqz0JBkBri3qu5PchJwMnB3zxqlVbXhwi8se9ndF71wgpVIyzfOqZdXAP8JnJJkT5LXdrPO5YFDOADPBXYkuQX4DHB+Vd07yYIlSQ/dOGfjbFqk/c9HtF0FXNW/LEnSJPkNWklqgGEvSQ0w7CWpAYa9JDXAsJekBhj2ktQAw16SGmDYS1IDDHtJaoBhL0kNMOwlqQGGvSQ1wLCXpAYY9pLUAMNekhpg2EtSAwx7SWqAYS9JDTDsJakBhr0kNcCwl6QGLBn2SbYm2Z9k51DbO5N8P8n27nb20Ly3JLkryR1JXjCtwiVJ4xtnz/6jwJkj2j9UVRu727UASZ4OnAs8o1vmH5IcMaliJUnLs2TYV9UNwL1jPt85wCer6udV9V/AXcBpPeqTJE1AnzH7C5Ls6IZ5ju7ajgO+N9RnT9f2IEk2J5lLMjc/P9+jDEnSUpYb9pcATwU2AnuBi7v2jOhbo56gqrZU1WxVzc7MzCyzDEnSOJYV9lW1r6rur6pfAJfxq6GaPcAJQ12PB+7pV6Ikqa9lhX2S9UMPXwosnKlzDXBukkclORE4GfhavxIlSX0duVSHJFcApwPrkuwB3gGcnmQjgyGa3cDrAKrq1iSfAr4FHABeX1X3T6d0SdK4lgz7qto0ovkjh+j/HuA9fYqSJE2W36CVpAYY9pLUAMNekhpg2EtSAwx7SWqAYS9JDTDsJakBhr0kNcCwl6QGGPaS1ADDXpIaYNhLUgMMe0lqgGEvSQ0w7CWpAYa9JDXAsJekBhj2ktQAw16SGmDYS1IDDHtJasCSYZ9ka5L9SXYOtb0/ye1JdiS5OslRXfuGJD9Lsr27XTrN4iVJ4xlnz/6jwJkHtW0Dfquqfhv4NvCWoXm7qmpjdzt/MmVKkvpYMuyr6gbg3oParquqA93DG4Hjp1CbJGlCJjFm/xrgi0OPT0zyjSRfSfKcxRZKsjnJXJK5+fn5CZQhSVpMr7BP8jbgAPDxrmkv8JSqOhV4E/CJJI8ftWxVbamq2aqanZmZ6VOGJGkJyw77JOcBLwL+rKoKoKp+XlU/7KZvAnYBT5tEoZKk5VtW2Cc5E3gz8OKq+ulQ+0ySI7rpk4CTgbsnUagkafmOXKpDkiuA04F1SfYA72Bw9s2jgG1JAG7szrx5LvCuJAeA+4Hzq+rekU8sSVoxS4Z9VW0a0fyRRfpeBVzVtyhJ0mT5DVpJaoBhL0kNMOwlqQGGvSQ1wLCXpAYY9pLUAMNekhpg2EtSAwx7SWqAYS9JDTDsJakBhr0kNcCwl6QGGPaS1ADDXpIaYNhLUgMMe0lqgGEvSQ0w7CWpAYa9JDXAsJekBowV9km2JtmfZOdQ2zFJtiW5s7s/umtPkg8nuSvJjiTPnFbxkqTxjLtn/1HgzIPaLgSur6qTgeu7xwBnASd3t83AJf3LlCT1MVbYV9UNwL0HNZ8DXN5NXw68ZKj9YzVwI3BUkvWTKFaStDx9xuyfVFV7Abr7Y7v244DvDfXb07U9QJLNSeaSzM3Pz/coQ5K0lGkcoM2ItnpQQ9WWqpqtqtmZmZkplCFJWtAn7PctDM909/u79j3ACUP9jgfu6bEeSVJPfcL+GuC8bvo84HND7a/qzsp5FnDfwnCPJGl1HDlOpyRXAKcD65LsAd4BXAR8Kslrge8CL++6XwucDdwF/BR49YRrliQ9RGOFfVVtWmTWGSP6FvD6PkVJkibLb9BKUgMMe0lqgGEvSQ0w7CWpAYa9JDXAsJekBhj2ktQAw16SGmDYS1IDDHtJaoBhL0kNMOwlqQGGvSQ1wLCXpAYY9pLUAMNekhpg2EtSAwx7SWqAYS9JDTDsJakBhr0kNeDI5S6Y5BTgyqGmk4C3A0cBfwnMd+1vraprl12hJKm3ZYd9Vd0BbARIcgTwfeBq4NXAh6rqAxOpUJLU26SGcc4AdlXVdyb0fJKkCZpU2J8LXDH0+IIkO5JsTXL0qAWSbE4yl2Rufn5+VBdJ0oT0DvskjwReDHy6a7oEeCqDIZ69wMWjlquqLVU1W1WzMzMzfcuQJB3CJPbszwJurqp9AFW1r6rur6pfAJcBp01gHZKkHiYR9psYGsJJsn5o3kuBnRNYhySph2WfjQOQ5NHAHwOvG2p+X5KNQAG7D5onSVoFvcK+qn4KPPGgtlf2qkiSNHF+g1aSGmDYS1IDDHtJaoBhL0kNMOwlqQGGvSQ1wLCXpAYY9pLUAMNekhpg2EtSAwx7SWqAYS9JDTDsJakBhr0kNcCwl6QGGPaS1ADDXpIaYNhLUgN6/VtCaaVsuPALvZbffdELJ1SJdHhyz16SGuCe/Srps6fqXqqkh6p32CfZDfwEuB84UFWzSY4BrgQ2ALuBV1TVj/quS5K0PJMaxvnDqtpYVbPd4wuB66vqZOD67rEkaZVMa8z+HODybvpy4CVTWo8kaQyTCPsCrktyU5LNXduTqmovQHd/7MELJdmcZC7J3Pz8/ATKkCQtZhIHaJ9dVfckORbYluT2cRaqqi3AFoDZ2dmaQB2SpEX03rOvqnu6+/3A1cBpwL4k6wG6+/191yNJWr5eYZ/kMUketzANPB/YCVwDnNd1Ow/4XJ/1SJL66TuM8yTg6iQLz/WJqvqXJF8HPpXktcB3gZf3XI8kqYdeYV9VdwO/M6L9h8AZfZ5bkjQ5Xi5Bkhrg5RIOQ14UTNJD5Z69JDXAsJekBhj2ktQAw16SGmDYS1IDDHtJaoCnXkpT5H8k01rhnr0kNcCwl6QGGPaS1ADDXpIa4AFaPSQecJQOT+7ZS1IDDHtJaoBhL0kNMOwlqQGGvSQ1wLCXpAZ46mWD+v5bQ0mHn2Xv2Sc5IcmXk9yW5NYkb+ja35nk+0m2d7ezJ1euJGk5+uzZHwD+uqpuTvI44KYk27p5H6qqD/QvT5I0CcsO+6raC+ztpn+S5DbguEkVJkmanIkcoE2yATgV+GrXdEGSHUm2Jjl6kWU2J5lLMjc/Pz+JMiRJi+gd9kkeC1wFvLGqfgxcAjwV2Mhgz//iUctV1Zaqmq2q2ZmZmb5lSJIOoVfYJ3kEg6D/eFV9FqCq9lXV/VX1C+Ay4LT+ZUqS+lj2mH2SAB8BbquqDw61r+/G8wFeCuzsV6IeLjzlU1o9fc7GeTbwSuCbSbZ3bW8FNiXZCBSwG3hdrwolSb31ORvnP4CMmHXt8suRJE2Dl0uQpAYY9pLUAMNekhpg2EtSAwx7SWqAYS9JDTDsJakBhr0kNcCwl6QGGPaS1ADDXpIaYNhLUgP6XPWyeV6yV9LhwrCXHob67ojsvuiFE6pEa4XDOJLUAPfspTXKYUJNknv2ktSA5vfs3XuS1ILmw17Sg/XZCfLg7trkMI4kNeBhsWfvUIwkHdrUwj7JmcDfAkcA/1hVF01rXZIeHhw+mp6pDOMkOQL4e+As4OnApiRPn8a6JElLm9ae/WnAXVV1N0CSTwLnAN+a0vokadUcDn+RpKom/6TJy4Azq+ovusevBH63qi4Y6rMZ2Nw9PAW4Y+KFjG8d8INVXP9DcbjUap2Td7jUap2Tdag6f7OqZsZ5kmnt2WdE2wN+q1TVFmDLlNb/kCSZq6rZ1a5jHIdLrdY5eYdLrdY5WZOqc1qnXu4BThh6fDxwz5TWJUlawrTC/uvAyUlOTPJI4FzgmimtS5K0hKkM41TVgSQXAF9icOrl1qq6dRrrmpA1MZw0psOlVuucvMOlVuucrInUOZUDtJKktcXLJUhSAwx7SWpAk2Gf5J1Jvp9ke3c7e5F+Zya5I8ldSS5c6Tq7Gt6f5PYkO5JcneSoRfrtTvLN7ueZW8H6DrmNkjwqyZXd/K8m2bBStQ3VcEKSLye5LcmtSd4wos/pSe4bek+8faXr7Oo45OuYgQ9323NHkmeuUp2nDG2r7Ul+nOSNB/VZlW2aZGuS/Ul2DrUdk2Rbkju7+6MXWfa8rs+dSc5bhTqn93mvquZuwDuBv1mizxHALuAk4JHALcDTV6HW5wNHdtPvBd67SL/dwLoVrm3JbQT8FXBpN30ucOUqbMP1wDO76ccB3x5R5+nA51e6tof6OgJnA19k8F2WZwFfXQM1HwH8N4Mv+Kz6NgWeCzwT2DnU9j7gwm76wlGfI+AY4O7u/uhu+ugVrnNqn/cm9+zH9MtLPlTV/wILl3xYUVV1XVUd6B7eyOA7C2vFONvoHODybvozwBlJRn3pbmqqam9V3dxN/wS4DThuJWuYoHOAj9XAjcBRSdavck1nALuq6jurXAcAVXUDcO9BzcPvw8uBl4xY9AXAtqq6t6p+BGwDzlzJOqf5eW857C/o/lTausifdMcB3xt6vIfVD4jXMNirG6WA65Lc1F2KYiWMs41+2ad7E98HPHFFqhuhG0Y6FfjqiNm/l+SWJF9M8owVLexXlnod1+L78lzgikXmrYVtCvCkqtoLg1/+wLEj+qy1bTvRz/vD4nr2oyT5V+DJI2a9DbgEeDeDDfZu4GIGG/YBTzFi2amcp3qoWqvqc12ftwEHgI8v8jTPrqp7khwLbEtye7fnME3jbKMV245LSfJY4CrgjVX144Nm38xgGOJ/umM4/wycvNI1svTruGa2J0D3pckXA28ZMXutbNNxrZltO43P+8M27KvqeeP0S3IZ8PkRs1bskg9L1dodKHoRcEZ1A3YjnuOe7n5/kqsZDLFMO+zH2UYLffYkORJ4Ag/+E3vqkjyCQdB/vKo+e/D84fCvqmuT/EOSdVW1ohfKGuN1XGuXIjkLuLmq9h08Y61s086+JOuram837LV/RJ89DI4zLDge+PcVqO0BpvV5b3IY56AxzpcCO0d0WxOXfMjgn8C8GXhxVf10kT6PSfK4hWkGB3lG/UyTNs42ugZYOKvhZcC/LfYGnpbuGMFHgNuq6oOL9HnywrGEJKcx+Gz8cOWqHPt1vAZ4VXdWzrOA+xaGJ1bJJhYZwlkL23TI8PvwPOBzI/p8CXh+kqO7od3nd20rZqqf92kdaV7LN+CfgG8COxi8CdZ37b8BXDvU72wGZ27sYjCkshq13sVgHHF7d7v04FoZnA1zS3e7dSVrHbWNgHd1b1aAXwc+3f0cXwNOWoVt+AcM/hzfMbQdzwbOB87v+lzQbbtbGBwY+/1VqHPk63hQnWHwj4F2de/h2dV4X3a1PJpBeD9hqG3VtymDXz57gf9jsLf+WgbHia4H7uzuj+n6zjL4T3oLy76me6/eBbx6Feqc2ufdyyVIUgOaHMaRpNYY9pLUAMNekhpg2EtSAwx7SWqAYS9JDTDsJakB/w/Q1OEKxjsPhAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = setup_spacy()\n",
    "df = read_data(\"../../../data/raw/raw_tweets.csv\")\n",
    "df = preprocess_data(df)\n",
    "df = add_spacy_features(df=df, nlp=nlp)\n",
    "df = additional_processing(df)\n",
    "emoji_dict = prepare_reference_emoji_list(nlp=nlp)\n",
    "df = add_polarity_scores(df, emoji_dict, emoji_column='unique_emoji')\n",
    "\n",
    "df = df[df.polarity_for_unique_emoji.map(bool)]\n",
    "\n",
    "df = sum_polarity(df)\n",
    "df = prepare_target(df)\n",
    "df = downsample_target(df)\n",
    "X,y = y_X_preparation(df)\n",
    "X_train, X_test, y_train, y_test = train_test_preparation_for_model(X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 16 is smaller than n_iter=40. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=40. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          model_name  \\\n",
      "0  LogisticRegression(C=1.0, class_weight=None, d...   \n",
      "1  KNeighborsClassifier(algorithm='auto', leaf_si...   \n",
      "2  RandomForestClassifier(bootstrap=True, ccp_alp...   \n",
      "3  SGDClassifier(alpha=0.0001, average=False, cla...   \n",
      "\n",
      "                                         best_param  best_score  \\\n",
      "0  {'solver': 'liblinear', 'penalty': 'l2', 'C': 1}    0.600000   \n",
      "1                               {'n_neighbors': 10}    0.562162   \n",
      "2          {'n_estimators': 30, 'max_features': 21}    0.605405   \n",
      "3                 {'average': True, 'alpha': 1e-05}    0.578378   \n",
      "\n",
      "                                      best_estimator  \n",
      "0  LogisticRegression(C=1, class_weight=None, dua...  \n",
      "1  KNeighborsClassifier(algorithm='auto', leaf_si...  \n",
      "2  (DecisionTreeClassifier(ccp_alpha=0.0, class_w...  \n",
      "3  SGDClassifier(alpha=1e-05, average=True, class...  \n"
     ]
    }
   ],
   "source": [
    "#lasso = Lasso()\n",
    "logistic = LogisticRegression()\n",
    "knn = KNeighborsClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "sgd = SGDClassifier()\n",
    "\n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "param_grid =[#{'alpha': np.linspace(0.00001, 1, 40)},\n",
    "             {'penalty' : ['l1', 'l2'],'C' : [0,1,5,10],'solver' : ['lbfgs','liblinear']},\n",
    "             {'n_neighbors':[1,3,5,10]},\n",
    "             {'n_estimators':list(range(10,101,10)),'max_features':list(range(6,32,5))},\n",
    "             {'average': [True, False],'alpha': np.linspace(0.00001, 1, 40)}]\n",
    "model_list = [logistic, knn, rfc,sgd]\n",
    "\n",
    "grid_search, grid_results,results  = random_search_best_model_parameters(scorer,param_grid,model_list,X_train, X_test, y_train, y_test)   \n",
    "results = pd.DataFrame(results) \n",
    "print(results)\n",
    "best_estimator = results['best_estimator'][results['best_score']==results['best_score'].max()].iloc[0]\n",
    "final_accuracy_test,final_accuracy_train = final_model(X_train, X_test, y_train, y_test,best_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5531914893617021, 0.9945945945945946)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_accuracy_test,final_accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Example code from me also using scikit learn's pipeline\n",
    "https://github.com/samanthaedds/datascience-challenge/blob/master/Bunch_code_SE.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
